{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import requests, re, json\r\n",
                "from bs4 import BeautifulSoup\r\n",
                "from tqdm import tqdm"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "\r\n",
                "class CoronaVirusSipder(object):\r\n",
                "    \r\n",
                "    def __init__(self):\r\n",
                "        self.home_url = \"https://ncov.dxy.cn/ncovh5/view/pneumonia\"\r\n",
                "        \r\n",
                "    def get_content_from_url(self, url):\r\n",
                "        response = requests.get(url)\r\n",
                "        return response.content.decode()\r\n",
                "    \r\n",
                "    def parse_home_page(self, homepage, tage_id):\r\n",
                "        soup = BeautifulSoup(homepage, 'lxml')\r\n",
                "        script = soup.find(id=tage_id)\r\n",
                "        text = script.string\r\n",
                "        \r\n",
                "        \r\n",
                "        json_str = re.findall(r'\\[.+\\]',text)[0]\r\n",
                "        \r\n",
                "        data = json.loads(json_str)\r\n",
                "        return data\r\n",
                "    \r\n",
                "    def load(self, path):\r\n",
                "        with open(path,encoding=\"utf-8\") as fp:\r\n",
                "            data = json.load(fp)\r\n",
                "        return data\r\n",
                "    \r\n",
                "    def save(self, data, path):\r\n",
                "        with open(path,\"w\",encoding='utf-8') as fp:\r\n",
                "            json.dump(data, fp, ensure_ascii=False)\r\n",
                "            \r\n",
                "    def crawl_last_day_corona_virus_of_china(self):\r\n",
                "        #1.获取首页内容\r\n",
                "        homepage = self.get_content_from_url(self.home_url)\r\n",
                "        #2.解析数据\r\n",
                "        last_day_corona_virus = self.parse_home_page(homepage, \"getAreaStat\")\r\n",
                "        #3.保存数据\r\n",
                "        self.save(last_day_corona_virus, \"data\\\\last_day_corona_virus_of_china.json\")\r\n",
                "        \r\n",
                "    def crawl_corona_virus_of_china(self):\r\n",
                "        \"\"\"\r\n",
                "        采集从1月23号以来全国各省的疫情数据\r\n",
                "        \"\"\"\r\n",
                "        #1.加载各国疫情数据\r\n",
                "        last_day_corona_virus = self.load(\"data\\\\last_day_corona_virus_of_china.json\")\r\n",
                "        corona_virus_of_china = []\r\n",
                "        #2.便利各国疫情数据statisticsData，获取统计的URL\r\n",
                "        for province in tqdm(last_day_corona_virus, \"采集1月23日以来各省信息\"):\r\n",
                "             #3.发送请求，获取各国从1月23日至今的json数据\r\n",
                "            statistic_data_url = province['statisticsData']\r\n",
                "            statistic_data_json_str = self.get_content_from_url(statistic_data_url)\r\n",
                "            #4.把json数据转换为Python数据，添加到列表中\r\n",
                "            statistic_data = json.loads(statistic_data_json_str)['data']\r\n",
                "            for one_day in statistic_data:\r\n",
                "                one_day['provinceName'] = province['provinceName']\r\n",
                "                one_day['provinceShortName'] = province['provinceShortName']\r\n",
                "            corona_virus_of_china.extend(statistic_data)\r\n",
                "        #5.保存列表数据到json文件\r\n",
                "        self.save(corona_virus_of_china, \"data\\\\corona_virus_of_china.json\")\r\n",
                "        \r\n",
                "        \r\n",
                "    def run(self):\r\n",
                "        self.crawl_last_day_corona_virus_of_china()\r\n",
                "        self.crawl_corona_virus_of_china()\r\n",
                "        \r\n",
                "        \r\n",
                "if __name__ == '__main__':\r\n",
                "    spider = CoronaVirusSipder()\r\n",
                "    spider.run()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "采集1月23日以来各省信息: 100%|██████████| 34/34 [00:38<00:00,  1.14s/it]\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}